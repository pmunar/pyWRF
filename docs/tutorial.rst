.. _tutorial:

Tutorial
========

In this tutorial we will cover the main steps to perform in order to analyze a small set of GDAS data.

We will assume that our working directory is under the /home/analysis/ path

It is assumed that we downloaded the GDAS data from 2012-01-01 until 2012-01-09 from the `UCAR download system <https://rda.ucar.edu/datasets/ds083.2/index.html#!cgi-bin/datasets/getSubset?dsnum=083.2&listAction=customize&_da=y&gindex=2>`__ by selecting all available parameters and a certain region of interest (we centered ours on the Canary islands, Spain).

Once we get the data, we will assume that we have stored it in the folder /home/wrf_analysis/data path

The outputs generated by the program are assumed to be in /home/analysis/ path

The first thing to do is to run the wrf_config script, in order to get a copy of the configuration file into the working directory.

.. code-block:: bash

    > wrf_config config_file.conf

Once we have the config file in the working directory we can open it with our favourite editor and edit its content.

For this case, we need to change the parameters in the Data section and in the Analysis section:

.. code-block:: bash

	[Data]

    data_path = /home/wrf_analysis/data
    output_path = /home/wrf_analysis/
	input_data_server = GDAS
	data_format = grib2
	hours_step = 6

	[Analysis]

	start_date = "2012-01-01_00_00"
	end_date   = "2012-01-07_00_00"
	num_domains = 3
	group_of_days = 5
	parallel = False
	#only useful if parallel = True
	ncores = 4
	tag = My_analysis_test


This way we told the program where our data is (data_path), where do we want the outputs (output_path), which is the format of the input GDAS files (data_format), the stard and end dates of the whole analysis and how many days do we want to analyze in a single push (group_of_days).

Now we just need to run another command to start the analysis:

.. code-block:: bash

    > wrf_analysis config_file.conf

If you run the command like this, with no optional parameters, it will run all the PREPROCESSING, the WRF scripts and the ARWpost scripts. At the end, it will copy all the output files into the wps_out, wrf_out and arwpost_out directories, under your working directory (/home/wrf_analysis/ in this case). If these directories do not exist, the program creates them.

The program, in this case, will take all the data and divide it into groups of 5 days (as specified by the group_of_days parameter), and analyze each group together. If the last group is shorter than the group_of_days parameter, the program simply takes the remaining data and analizes it.

If you want to run only a part of the program, e.g. the PREPROCESSING, you can call it like this:

.. code-block:: bash

    > wrf_analysis config_file.conf -wps

The same can be done if you want just to run the wrf or the arwpost parts of the analysis. Note that in these cases, the files that the program needs, need to be in the corresponding folder in the $WRF_DIR.

After completion, the outputs will not be moved to the output directory, since they might be needed for other parts of the analysis process.

In order to move all outputs into the working directory, you can do:

.. code-block:: bash

    > wrf_analysis config_file.conf -clean

Since the postprocessing and outputs that WRF can deliver are too wide, pyWRF only provides a tool for postprocessing that was needed by us when we developed it. 
If you need other features, you can `contact me <pere.munar@uab.cat>`__ or you can try to include it into the code yourself, by creating a new branch on the GitHub repository. It will be very welcome!

There is another Python package available that performs astounding plots and visualizations for the outputs of the WRF software. It is called `wrf-python <https://github.com/NCAR/wrf-python>`__ and you can find its `documentation here <https://wrf-python.readthedocs.io/en/latest/index.html>`__. 